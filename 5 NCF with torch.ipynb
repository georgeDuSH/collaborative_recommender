{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 609, Items: 9562. Sparsity: 0.983\n",
      "User reduced from 609 to 608\n"
     ]
    }
   ],
   "source": [
    "from recom.datasets import load_ml_small_rating\n",
    "\n",
    "# load data\n",
    "# not that I use leave-one-out method to construct the testing set, where\n",
    "# the latest rated item is masked and added to the testing set as an evaluation.\n",
    "dataset = load_ml_small_rating(need_raw=True, time_ord=True, test_perc=0.1)\n",
    "\n",
    "# load features\n",
    "ratings = dataset['raw']    \n",
    "ratings_train_dict = dataset['train_dict']\n",
    "ratings_test_dict = dataset['test_dict']\n",
    "n_user = dataset['n_user']\n",
    "n_item = dataset['n_item']\n",
    "user2ix = dataset['user2ix']\n",
    "ix2user = dataset['ix2user']\n",
    "item2ix = dataset['item2ix']\n",
    "ix2item = dataset['ix2item']\n",
    "\n",
    "del dataset\n",
    "\n",
    "print(f'Users: {n_user}, Items: {n_item}. Sparsity: {round(1-len(ratings)/n_user/n_item, 4)}')\n",
    "print(f'User reduced from {len(user2ix.keys())} to {len(ratings_train_dict.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3903],\n",
       "        [0.4543],\n",
       "        [0.3995],\n",
       "        [0.4731],\n",
       "        [0.5191]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor, LongTensor, sigmoid\n",
    "from torch.nn.functional import logsigmoid\n",
    "\n",
    "\n",
    "class GMF(nn.Module):\n",
    "    \"\"\" General Matrix Factorization \"\"\"\n",
    "    def __init__(self, n_user, n_item, k_dim, activation='sigmoid') -> None:\n",
    "        assert activation in ['sigmoid', 'identity'], f'Invalid activation function for GMF {activation}.'\n",
    "\n",
    "        super(GMF, self).__init__()\n",
    "        # embeddings\n",
    "        self.embedding_user = nn.Embedding(n_user, k_dim)\n",
    "        self.embedding_item = nn.Embedding(n_item, k_dim)\n",
    "        # weights\n",
    "        self.linear = nn.Linear(k_dim, 1)\n",
    "        # activation\n",
    "        self.activation = sigmoid if activation=='sigmoid' else nn.Identity()\n",
    "        # init param\n",
    "        nn.init.normal_(self.embedding_user.weight, mean=0, std=1)\n",
    "        nn.init.normal_(self.embedding_item.weight, mean=0, std=1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_emb = self.embedding_user(user)\n",
    "        item_emb = self.embedding_item(item)\n",
    "\n",
    "        return self.activation(\n",
    "            self.linear(user_emb*item_emb)\n",
    "        )\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Multi-Layer Perceptron \"\"\"\n",
    "    def __init__(self, n_user, n_item, k_dim \n",
    "                     , num_layers, p_dropout) -> None:\n",
    "        \n",
    "        super(GMF, self).__init__()\n",
    "        # embeddings\n",
    "        self.embedding_user = nn.Embedding(n_user, k_dim)\n",
    "        self.embedding_item = nn.Embedding(n_item, k_dim)\n",
    "\n",
    "\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    \"\"\" Neural Matrix Factorization: Fusion of GMF and MLP \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "gmf = GMF(10, 20, 10, activation='sigmoid')\n",
    "\n",
    "users = LongTensor([0,1,2,3,4])\n",
    "items = LongTensor([1,1,1,3,4])\n",
    "\n",
    "gmf(users, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0914, -0.0433, -0.0532, -0.0686,  0.0317,  0.0350, -0.0755, -0.0882,\n",
       "          0.1961, -0.0904],\n",
       "        [-0.0891, -0.0040,  0.0249,  0.0659,  0.0359,  0.0736, -0.0962,  0.1316,\n",
       "          0.1331,  0.0945],\n",
       "        [-0.2859,  0.0470,  0.0252,  0.0615,  0.1043, -0.1550, -0.0327,  0.1001,\n",
       "         -0.0188, -0.0043],\n",
       "        [-0.0118,  0.1401, -0.0508,  0.2316,  0.0492,  0.0949, -0.0806,  0.1010,\n",
       "          0.1429, -0.0956],\n",
       "        [-0.0451, -0.0537,  0.0810, -0.0042,  0.0039, -0.0303, -0.0387, -0.0049,\n",
       "          0.0189, -0.0301]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmf.embedding_user(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0420,  0.1218, -0.1399, -0.2115,  0.1238, -0.0234,  0.0315,  0.1099,\n",
       "          0.1311,  0.0386],\n",
       "        [-0.0420,  0.1218, -0.1399, -0.2115,  0.1238, -0.0234,  0.0315,  0.1099,\n",
       "          0.1311,  0.0386],\n",
       "        [-0.0420,  0.1218, -0.1399, -0.2115,  0.1238, -0.0234,  0.0315,  0.1099,\n",
       "          0.1311,  0.0386],\n",
       "        [ 0.0295,  0.0525,  0.0866, -0.1791,  0.0441, -0.0125, -0.0089, -0.0674,\n",
       "         -0.2411,  0.0215],\n",
       "        [ 0.0689,  0.1505,  0.0462, -0.1141, -0.0659, -0.1473,  0.3071,  0.1171,\n",
       "          0.1345,  0.1276]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmf.embedding_item(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0038, -0.0053,  0.0074,  0.0145,  0.0039, -0.0008, -0.0024, -0.0097,\n",
       "          0.0257, -0.0035],\n",
       "        [ 0.0037, -0.0005, -0.0035, -0.0139,  0.0044, -0.0017, -0.0030,  0.0145,\n",
       "          0.0175,  0.0036],\n",
       "        [ 0.0120,  0.0057, -0.0035, -0.0130,  0.0129,  0.0036, -0.0010,  0.0110,\n",
       "         -0.0025, -0.0002],\n",
       "        [-0.0003,  0.0074, -0.0044, -0.0415,  0.0022, -0.0012,  0.0007, -0.0068,\n",
       "         -0.0344, -0.0021],\n",
       "        [-0.0031, -0.0081,  0.0037,  0.0005, -0.0003,  0.0045, -0.0119, -0.0006,\n",
       "          0.0025, -0.0038]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ews = gmf.embedding_user(users)*gmf.embedding_item(items)\n",
    "ews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0020,  0.0078,  0.0021,  0.0036, -0.0032], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gmf.linear.weight.data[0] * ews).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0261,  0.0211,  0.0251, -0.0805, -0.0165], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ews.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.2394], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmf.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2281,  0.0944, -0.2378, -0.2669,  0.1129,  0.0884,  0.2206, -0.0763,\n",
       "          0.2844,  0.1728]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmf.linear.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00499476119a38fdac92034240d7ef2fa4f5985bf02d398f0fd3693908f0286e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
