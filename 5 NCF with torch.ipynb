{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 609, Items: 9562. Sparsity: 0.983\n",
      "User reduced from 609 to 608\n"
     ]
    }
   ],
   "source": [
    "from recom.datasets import load_ml_small_rating\n",
    "\n",
    "# load data\n",
    "# not that I use leave-one-out method to construct the testing set, where\n",
    "# the latest rated item is masked and added to the testing set as an evaluation.\n",
    "dataset = load_ml_small_rating(need_raw=True, time_ord=True, test_perc=0.1)\n",
    "\n",
    "# load features\n",
    "ratings = dataset['raw']    \n",
    "ratings_train_dict = dataset['train_dict']\n",
    "ratings_test_dict = dataset['test_dict']\n",
    "n_user = dataset['n_user']\n",
    "n_item = dataset['n_item']\n",
    "user2ix = dataset['user2ix']\n",
    "ix2user = dataset['ix2user']\n",
    "item2ix = dataset['item2ix']\n",
    "ix2item = dataset['ix2item']\n",
    "\n",
    "del dataset\n",
    "\n",
    "print(f'Users: {n_user}, Items: {n_item}. Sparsity: {round(1-len(ratings)/n_user/n_item, 4)}')\n",
    "print(f'User reduced from {len(user2ix.keys())} to {len(ratings_train_dict.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor, LongTensor, sigmoid, concat\n",
    "from torch.nn.functional import logsigmoid\n",
    "\n",
    "\n",
    "class GMF(nn.Module):\n",
    "    \"\"\" General Matrix Factorization \"\"\"\n",
    "    def __init__(self, n_user, n_item, k_dim) -> None:\n",
    "        super(GMF, self).__init__()\n",
    "        # embeddings\n",
    "        self.embedding_user = nn.Embedding(n_user, k_dim)\n",
    "        self.embedding_item = nn.Embedding(n_item, k_dim)\n",
    "        # weights\n",
    "        self.linear = nn.Linear(k_dim, 1)\n",
    "        # activation\n",
    "        self.sigmoid = sigmoid\n",
    "        # init param\n",
    "        nn.init.normal_(self.embedding_user.weight, mean=0, std=1)\n",
    "        nn.init.normal_(self.embedding_item.weight, mean=0, std=1)\n",
    "\n",
    "    def pred_score(self, user, item):\n",
    "        user_emb = self.embedding_user(user)\n",
    "        item_emb = self.embedding_item(item)\n",
    "\n",
    "        return user_emb*item_emb\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        return self.sigmoid(self.linear(\n",
    "            self.pred_score(user, item)\n",
    "        ))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Multi-Layer Perceptron \"\"\"\n",
    "    def __init__(self, n_user, n_item, out_dim \n",
    "                     , num_layers, p_dropout=0.05) -> None:\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        # embeddings\n",
    "        emb_dim = out_dim * num_layers\n",
    "        self.embedding_user = nn.Embedding(n_user, emb_dim)\n",
    "        self.embedding_item = nn.Embedding(n_item, emb_dim)\n",
    "        # MLP blocks\n",
    "        MLP_Layer = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = 2 * out_dim * (num_layers-i)\n",
    "            MLP_Layer.append(nn.Dropout(p_dropout))\n",
    "            MLP_Layer.append(nn.Linear(input_size, input_size//2))\n",
    "            MLP_Layer.append(nn.ReLU())        \n",
    "        self.MLP_Layer = nn.Sequential(*MLP_Layer) # Open the MLP layer\n",
    "        # Last linear layer\n",
    "        self.Linear = nn.Linear(out_dim, 1)\n",
    "        # activations\n",
    "        self.sigmoid = sigmoid\n",
    "        # init param\n",
    "        nn.init.normal_(self.embedding_user.weight, mean=0, std=1)\n",
    "        nn.init.normal_(self.embedding_item.weight, mean=0, std=1)\n",
    "\n",
    "    def pred_score(self, user, item):\n",
    "        user_emb = self.embedding_user(user)\n",
    "        item_emb = self.embedding_item(item)\n",
    "        mlp_input = concat([user_emb, item_emb], 1)\n",
    "\n",
    "        mlp_out = self.MLP_Layer(mlp_input)\n",
    "\n",
    "        return mlp_out\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        return self.sigmoid(self.Linear(\n",
    "            self.pred_score(user, item)\n",
    "        ))\n",
    "\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    \"\"\" Neural Matrix Factorization: Fusion of GMF and MLP \"\"\"\n",
    "    def __init__(self, n_user, n_item, k_dim \n",
    "                     , num_mlp_layers, p_dropout=0.05) -> None:\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.GMF = GMF(n_user, n_item, k_dim)\n",
    "        self.MLP = MLP(n_user, n_item, k_dim, num_mlp_layers, p_dropout)\n",
    "        self.Linear = nn.Linear(2*k_dim, 1)\n",
    "        self.sigmoid = sigmoid\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        GMF_output = self.GMF.pred_score(user, item)\n",
    "        MLP_output = self.MLP.pred_score(user, item)\n",
    "\n",
    "        return self.sigmoid(self.Linear(\n",
    "            concat([GMF_output, MLP_output], 1)\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_pointwise_mf_loader(rat_dict, items, batch_size, neg_size=None\n",
    "                              , random_sampling=True, user_size=None, pos_size=None\n",
    "                              , user_neg_dict=None):\n",
    "    from random import choices\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    if not isinstance(items, set):\n",
    "        all_items = set(items)\n",
    "    all_items = items\n",
    "    \n",
    "    train_data = []\n",
    "\n",
    "    if not random_sampling: # goover all dataset\n",
    "        for user in rat_dict:\n",
    "            pos_items = list(rat_dict[user].keys())\n",
    "            neg_candidates = list(all_items - set(pos_items)) if user_neg_dict is None \\\n",
    "                             else user_neg_dict[user]\n",
    "            neg_items = choices(neg_candidates, k=len(pos_items)*neg_size)\n",
    "            # add positive and negative candidates to training data\n",
    "            train_data.extend(zip([1]*len(pos_items), [user]*len(pos_items), pos_items))\n",
    "            train_data.extend(zip([0]*len(neg_items), [user]*len(neg_items), neg_items))     \n",
    "                \n",
    "    else:\n",
    "        users = choices(list(rat_dict.keys()), k=user_size)\n",
    "        for user in users:\n",
    "            neg_candidates = list(all_items - set(rat_dict[user])) if user_neg_dict is None \\\n",
    "                             else user_neg_dict[user]\n",
    "            pos_items = choices(list(rat_dict[user].keys()), k=pos_size)\n",
    "            neg_items = choices(neg_candidates, k=pos_size*neg_size)\n",
    "            # add positive and negative candidates to training data\n",
    "            train_data.extend(zip([1]*len(pos_items), [user]*len(pos_items), pos_items))\n",
    "            train_data.extend(zip([0]*len(neg_items), [user]*len(neg_items), neg_items))     \n",
    "            \n",
    "    return DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# things for sampling\n",
    "items = list(range(n_item))\n",
    "# first define the dictionary can accelerate sampling efficiency\n",
    "user_neg_dict = {\n",
    "    u:list(set(items)-set(ratings_train_dict[u].keys()))\n",
    "     for u in ratings_train_dict\n",
    "}\n",
    "\n",
    "# DL1: roll over all data\n",
    "dl_roll = naive_pointwise_mf_loader(\n",
    "    rat_dict=ratings_train_dict, items=items, user_neg_dict=user_neg_dict\n",
    "    , random_sampling=False, neg_size=4\n",
    "    , batch_size=128\n",
    ")\n",
    "\n",
    "# DL2: sampling by parameters\n",
    "dl_sample = naive_pointwise_mf_loader(\n",
    "    rat_dict=ratings_train_dict, items=items, user_neg_dict=user_neg_dict\n",
    "    , random_sampling=True, neg_size=4\n",
    "    , user_size=256, pos_size=64\n",
    "    , batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Shenghui\\Desktop\\my_repository\\recommender\\5 NCF with torch.ipynb 单元格 4\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shenghui/Desktop/my_repository/recommender/5%20NCF%20with%20torch.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gmf \u001b[39m=\u001b[39m GMF(n_user\u001b[39m=\u001b[39mn_user, n_item\u001b[39m=\u001b[39mn_item, k_dim\u001b[39m=\u001b[39mK_DIM)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shenghui/Desktop/my_repository/recommender/5%20NCF%20with%20torch.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m target, user, item \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(dl_roll)[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Shenghui/Desktop/my_repository/recommender/5%20NCF%20with%20torch.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(compute_device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shenghui/Desktop/my_repository/recommender/5%20NCF%20with%20torch.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m user \u001b[39m=\u001b[39m autograd\u001b[39m.\u001b[39mVariable(LongTensor(user))\u001b[39m.\u001b[39mto(compute_device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shenghui/Desktop/my_repository/recommender/5%20NCF%20with%20torch.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m item \u001b[39m=\u001b[39m autograd\u001b[39m.\u001b[39mVariable(LongTensor(item))\u001b[39m.\u001b[39mto(compute_device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_device' is not defined"
     ]
    }
   ],
   "source": [
    "gmf = GMF(n_user=n_user, n_item=n_item, k_dim=K_DIM)\n",
    "\n",
    "\n",
    "\n",
    "target, user, item = list(dl_roll)[0]\n",
    "\n",
    "target = target.float().to(compute_device)\n",
    "user = autograd.Variable(LongTensor(user)).to(compute_device)\n",
    "item = autograd.Variable(LongTensor(item)).to(compute_device)\n",
    "\n",
    "gmf(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time: 1.95,  /Average train loss 0.34854\n",
      "Epoch: 2, Time: 1.84,  /Average train loss 0.23749\n",
      "Epoch: 3, Time: 1.72,  /Average train loss 0.23475\n",
      "Epoch: 4, Time: 1.88,  /Average train loss 0.23421\n",
      "Epoch: 5, Time: 1.77,  /Average train loss 0.23469\n",
      "Epoch: 6, Time: 1.8,  /Average train loss 0.23494\n",
      "Epoch: 7, Time: 1.88,  /Average train loss 0.23531\n",
      "Epoch: 8, Time: 2.13,  /Average train loss 0.23558\n",
      "Epoch: 9, Time: 2.29,  /Average train loss 0.236\n",
      "Epoch: 10, Time: 1.98,  /Average train loss 0.23643\n",
      "Epoch: 11, Time: 1.8,  /Average train loss 0.23637\n",
      "Epoch: 12, Time: 1.9,  /Average train loss 0.23697\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch import autograd, LongTensor, FloatTensor, device\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def log_loss(input, target):\n",
    "    loss = nn.BCELoss()\n",
    "    return loss(input, target)\n",
    "\n",
    "\n",
    "def train_model(model, opt, rat_train, n_items\n",
    "                , use_random_sampling=True, neg_size=4\n",
    "                , user_size=256, pos_size=32\n",
    "                , use_cuda=False, n_epochs=64, batch_size=256\n",
    "                , test_dict=None, metrics=None, k=None\n",
    "                , report_interval=1):\n",
    "\n",
    "    if use_cuda:\n",
    "        compute_device = device('cuda')\n",
    "        model.cuda()\n",
    "    else:\n",
    "        compute_device = device('cpu')\n",
    "\n",
    "    # things for sampling\n",
    "    items = list(range(n_item))\n",
    "    # first define the dictionary can accelerate sampling efficiency\n",
    "    user_neg_dict = {\n",
    "        u:list(set(items)-set(rat_train[u].keys()))\n",
    "        for u in rat_train}\n",
    "\n",
    "    train_loss_by_ep = []\n",
    "    test_loss_by_ep = []\n",
    "\n",
    "    # place holder for metric\n",
    "    # if metrics is not None:\n",
    "    #     metrics_at_k = {metric[0]:[] for metric in metrics.items()} \n",
    "    #     test_cands = generate_testing_candidates(ratings_train_dict, n_item, n=100)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        train_data = dl_sample = naive_pointwise_mf_loader(\n",
    "                        rat_dict=rat_train, items=items\n",
    "                        , user_neg_dict=user_neg_dict\n",
    "                        , random_sampling=use_random_sampling\n",
    "                        , neg_size=neg_size\n",
    "                        , user_size=user_size, pos_size=pos_size\n",
    "                        , batch_size=batch_size\n",
    "                    )\n",
    "\n",
    "        ep_loss = []\n",
    "        for i, batch in enumerate(train_data):\n",
    "            target, user, item = batch\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            target = target.float().to(compute_device)\n",
    "            user = autograd.Variable(LongTensor(user)).to(compute_device)\n",
    "            item = autograd.Variable(LongTensor(item)).to(compute_device)\n",
    "\n",
    "            preds = model(user, item).squeeze(-1)\n",
    "            loss = log_loss(input=preds, target=target) # todo\n",
    "            # print(loss)\n",
    "            loss.sum().backward()\n",
    "            opt.step()\n",
    "            ep_loss.append(loss.data.to('cpu'))\n",
    "            \n",
    "        train_loss_by_ep.append(np.mean(ep_loss))\n",
    "\n",
    "        # compute testing result\n",
    "        # preds = bpr.pred_all().to('cpu')\n",
    "        # for u in results:\n",
    "        #     pred_items = Tensor([preds[u][i] for i in test_cands[u]])\n",
    "        #     results[u] = [test_cands[u][ix] for ix in argsort(-pred_items)[:100]]\n",
    "\n",
    "        # for metric in metrics_at_k:\n",
    "        #     metrics_at_k[metric].append(metrics[metric](k, ratings_test_dict, results))\n",
    "\n",
    "        if report_interval > 0 \\\n",
    "                and ((epoch+1) % report_interval == 0):\n",
    "            \n",
    "            t1=time.time()\n",
    "            print(f'Epoch: {epoch+1}, Time: {round(t1-t0,2)},  /Average train loss {round(sum(train_loss_by_ep[-report_interval:])/report_interval, 5)}')\n",
    "            # average_metrics = {metric:round(sum(metrics_at_k[metric][-report_interval:])/report_interval, 5) for metric in metrics_at_k}\n",
    "            # test_metrics = ' '.join(f'{m_items[0]}:{m_items[1]}' for m_items in average_metrics.items())\n",
    "            # print(f'\\t\\t\\t/Average test metric at {k}: {test_metrics}')\n",
    "            t0=time.time()\n",
    "\n",
    "    # finish traniing, send to cpu anyway\n",
    "    model = model.to('cpu') \n",
    "\n",
    "    # if test_dict is not None:\n",
    "    #     return model, train_loss_by_ep, metrics_at_k\n",
    "\n",
    "    return model, train_loss_by_ep, test_loss_by_ep\n",
    "\n",
    "\n",
    "# from recom.model.pairwise import BPR\n",
    "from torch import optim\n",
    "# from recom.eval.metrics import map, hit_rate, ndcg\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "K_DIM=12\n",
    "NEG_SIZE=16 # 4\n",
    "USER_SIZE=128\n",
    "POS_SIZE=64\n",
    "USE_CUDA=True\n",
    "N_EPOCHES=12\n",
    "BATCH_SIZE=512\n",
    "INTERVAL=1\n",
    "\n",
    "# model = GMF(n_user=n_user, n_item=n_item, k_dim=K_DIM)\n",
    "# model = MLP(n_user=n_user, n_item=n_item, out_dim=K_DIM, num_layers=2)\n",
    "model = NeuMF(n_user=n_user, n_item=n_item, k_dim=K_DIM, num_mlp_layers=2)\n",
    "\n",
    "# leave one out\n",
    "# opt = optim.Adam(bpr.parameters(), lr=0.002, weight_decay=0.1) # :mAP:0.0008 hit_rate:0.07566 ndcg:0.00919\n",
    "# leave last 10% chronologically\n",
    "# optim.Adam(bpr.parameters(), lr=0.001, weight_decay=0.05) # mAP:0.00445 hit_rate:0.12336 ndcg:0.0537\n",
    "opt = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.05)\n",
    "model, train_loss_by_ep, test_rmse_by_ep = train_model(\n",
    "    model=model, opt=opt, rat_train=ratings_train_dict\n",
    "    , n_items=n_item, use_random_sampling=True\n",
    "    , neg_size=NEG_SIZE\n",
    "    , user_size=USER_SIZE, pos_size=POS_SIZE\n",
    "    , use_cuda=USE_CUDA, n_epochs=N_EPOCHES, batch_size=BATCH_SIZE\n",
    "    , report_interval=INTERVAL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0038, -0.0053,  0.0074,  0.0145,  0.0039, -0.0008, -0.0024, -0.0097,\n",
       "          0.0257, -0.0035],\n",
       "        [ 0.0037, -0.0005, -0.0035, -0.0139,  0.0044, -0.0017, -0.0030,  0.0145,\n",
       "          0.0175,  0.0036],\n",
       "        [ 0.0120,  0.0057, -0.0035, -0.0130,  0.0129,  0.0036, -0.0010,  0.0110,\n",
       "         -0.0025, -0.0002],\n",
       "        [-0.0003,  0.0074, -0.0044, -0.0415,  0.0022, -0.0012,  0.0007, -0.0068,\n",
       "         -0.0344, -0.0021],\n",
       "        [-0.0031, -0.0081,  0.0037,  0.0005, -0.0003,  0.0045, -0.0119, -0.0006,\n",
       "          0.0025, -0.0038]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ews = gmf.embedding_user(users)*gmf.embedding_item(items)\n",
    "ews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0020,  0.0078,  0.0021,  0.0036, -0.0032], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gmf.linear.weight.data[0] * ews).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0261,  0.0211,  0.0251, -0.0805, -0.0165], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ews.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.2394], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmf.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2281,  0.0944, -0.2378, -0.2669,  0.1129,  0.0884,  0.2206, -0.0763,\n",
       "          0.2844,  0.1728]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmf.linear.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00499476119a38fdac92034240d7ef2fa4f5985bf02d398f0fd3693908f0286e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
