{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 610, Items: 9724. Sparsity: 0.983\n",
      "User reduced from 610 to 607\n"
     ]
    }
   ],
   "source": [
    "from recom.datasets import load_ml_small_rating\n",
    "\n",
    "# load data\n",
    "dataset = load_ml_small_rating(need_raw=True)\n",
    "\n",
    "# load features\n",
    "ratings = dataset['raw']\n",
    "ratings_train_dict = dataset['train_dict']\n",
    "ratings_test_dict = dataset['test_dict']\n",
    "n_user = dataset['n_user']\n",
    "n_item = dataset['n_item']\n",
    "user2ix = dataset['user2ix']\n",
    "ix2user = dataset['ix2user']\n",
    "item2ix = dataset['item2ix']\n",
    "ix2item = dataset['ix2item']\n",
    "\n",
    "del dataset\n",
    "\n",
    "print(f'Users: {n_user}, Items: {n_item}. Sparsity: {round(1-len(ratings)/n_user/n_item, 4)}')\n",
    "print(f'User reduced from {len(user2ix.keys())} to {len(ratings_train_dict.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import Tensor, LongTensor\n",
    "\n",
    "\n",
    "class ItemAutoEncoder(nn.Module):\n",
    "    def __init__(self, n_user, hidden_dim=64\n",
    "                     , encode_actv_func=nn.Sigmoid\n",
    "                     , decode_actv_func=nn.Identity\n",
    "                     , dropout=0.05) -> None:\n",
    "        super(ItemAutoEncoder, self).__init__()\n",
    "        # encoder: (n_user, hidden_dim)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_user, hidden_dim, bias=True)\n",
    "            , encode_actv_func()\n",
    "        )\n",
    "        # decoder: (hidden_dim, n_user)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, n_user, bias=True)\n",
    "            , decode_actv_func()\n",
    "        )\n",
    "        # dropout to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def pred(self, input):\n",
    "\n",
    "        hidden = self.dropout(self.encoder(input))\n",
    "        ratings = self.decoder(hidden)\n",
    "        \n",
    "        return ratings\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Only for training.\n",
    "            We will use mask in this part to control gradients.\n",
    "        \"\"\"\n",
    "        ratings = self.pred(input)\n",
    "        # we contrain the output value to zero\n",
    "        # , which, mathematically, can control the gradient of \n",
    "        # non-observed rating-related params to zero.\n",
    "        mask = (input!=0)\n",
    "\n",
    "        return ratings * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 2.5000, 3.0000, 5.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 2.0000, 0.0000, 0.0000],\n",
       "        [4.0000, 0.0000, 0.0000,  ..., 2.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rating_vectorize(rat_dict, n_user, n_item, view='item'):\n",
    "    from torch import zeros\n",
    "\n",
    "    rat_mat = zeros(n_user, n_item)\n",
    "    for u in rat_dict:\n",
    "        for i in rat_dict[u]:\n",
    "            rat_mat[u, i] = rat_dict[u][i]\n",
    "    \n",
    "    if view == 'item':\n",
    "        return rat_mat.T\n",
    "    else:\n",
    "        return rat_mat\n",
    "\n",
    "train_mat = rating_vectorize(ratings_train_dict, n_user=n_user, n_item=n_item)\n",
    "train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autorec_data_loader(rat_dict, n_user, n_item, view='item'\n",
    "                        , use_sampling=False, obj_size=512\n",
    "                        , batch_size=256):\n",
    "    from torch.utils.data import DataLoader\n",
    "    from random import choices\n",
    "    from torch import LongTensor\n",
    "\n",
    "    rating_mat = rating_vectorize(rat_dict, n_user, n_item, view)\n",
    "\n",
    "    if use_sampling:\n",
    "        by_axis = n_item if view=='item' else n_user\n",
    "        rand_ixs = choices(list(range(by_axis)), obj_size)\n",
    "\n",
    "        return DataLoader(dataset=rating_mat[LongTensor(rand_ixs)]\n",
    "                          , batch_size=batch_size\n",
    "                          , shuffle=True)\n",
    "    else:\n",
    "        return DataLoader(dataset=rating_mat, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dl = autorec_data_loader(ratings_train_dict, n_user, n_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time: 30.91, /Average train loss 3.03924\n",
      "\t\t\t/Average test loss 2.44192\n",
      "Epoch: 2, Time: 12.67, /Average train loss 1.66055\n",
      "\t\t\t/Average test loss 1.24215\n",
      "Epoch: 3, Time: 1.0, /Average train loss 1.08988\n",
      "\t\t\t/Average test loss 0.96048\n",
      "Epoch: 4, Time: 0.9, /Average train loss 1.01298\n",
      "\t\t\t/Average test loss 0.92298\n",
      "Epoch: 5, Time: 0.92, /Average train loss 0.99674\n",
      "\t\t\t/Average test loss 0.91355\n",
      "Epoch: 6, Time: 0.85, /Average train loss 0.98642\n",
      "\t\t\t/Average test loss 0.90529\n",
      "Epoch: 7, Time: 0.87, /Average train loss 0.9732\n",
      "\t\t\t/Average test loss 0.89176\n",
      "Epoch: 8, Time: 0.85, /Average train loss 0.95789\n",
      "\t\t\t/Average test loss 0.88146\n",
      "Epoch: 9, Time: 0.85, /Average train loss 0.94281\n",
      "\t\t\t/Average test loss 0.86179\n",
      "Epoch: 10, Time: 0.95, /Average train loss 0.92764\n",
      "\t\t\t/Average test loss 0.84772\n",
      "Epoch: 11, Time: 0.85, /Average train loss 0.91337\n",
      "\t\t\t/Average test loss 0.83214\n",
      "Epoch: 12, Time: 0.83, /Average train loss 0.89985\n",
      "\t\t\t/Average test loss 0.82036\n",
      "Epoch: 13, Time: 0.9, /Average train loss 0.88839\n",
      "\t\t\t/Average test loss 0.81415\n",
      "Epoch: 14, Time: 0.88, /Average train loss 0.87929\n",
      "\t\t\t/Average test loss 0.79944\n",
      "Epoch: 15, Time: 0.87, /Average train loss 0.87063\n",
      "\t\t\t/Average test loss 0.79189\n",
      "Epoch: 16, Time: 0.91, /Average train loss 0.86254\n",
      "\t\t\t/Average test loss 0.78787\n",
      "Epoch: 17, Time: 0.87, /Average train loss 0.85625\n",
      "\t\t\t/Average test loss 0.78417\n",
      "Epoch: 18, Time: 0.92, /Average train loss 0.8516\n",
      "\t\t\t/Average test loss 0.77824\n",
      "Epoch: 19, Time: 0.97, /Average train loss 0.8446\n",
      "\t\t\t/Average test loss 0.77839\n",
      "Epoch: 20, Time: 0.83, /Average train loss 0.83815\n",
      "\t\t\t/Average test loss 0.77604\n",
      "Epoch: 21, Time: 0.87, /Average train loss 0.8333\n",
      "\t\t\t/Average test loss 0.76773\n",
      "Epoch: 22, Time: 0.88, /Average train loss 0.82748\n",
      "\t\t\t/Average test loss 0.77451\n",
      "Epoch: 23, Time: 0.86, /Average train loss 0.82303\n",
      "\t\t\t/Average test loss 0.77206\n",
      "Epoch: 24, Time: 0.86, /Average train loss 0.81745\n",
      "\t\t\t/Average test loss 0.76679\n",
      "Epoch: 25, Time: 0.85, /Average train loss 0.81265\n",
      "\t\t\t/Average test loss 0.76095\n",
      "Epoch: 26, Time: 0.83, /Average train loss 0.80784\n",
      "\t\t\t/Average test loss 0.76278\n",
      "Epoch: 27, Time: 0.88, /Average train loss 0.80193\n",
      "\t\t\t/Average test loss 0.76194\n",
      "Epoch: 28, Time: 0.84, /Average train loss 0.79653\n",
      "\t\t\t/Average test loss 0.76939\n",
      "Epoch: 29, Time: 0.93, /Average train loss 0.79185\n",
      "\t\t\t/Average test loss 0.76737\n",
      "Epoch: 30, Time: 0.8, /Average train loss 0.78682\n",
      "\t\t\t/Average test loss 0.77188\n",
      "Epoch: 31, Time: 0.84, /Average train loss 0.78148\n",
      "\t\t\t/Average test loss 0.77446\n",
      "Epoch: 32, Time: 0.82, /Average train loss 0.77732\n",
      "\t\t\t/Average test loss 0.77352\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch import autograd, LongTensor, device\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "def maskedSE(input:Tensor, target:Tensor):\n",
    "    mask = (target!=0)\n",
    "\n",
    "    return ((input[mask]-target[mask])**2)\n",
    "\n",
    "rating_mat = rating_vectorize(ratings_train_dict, n_user, n_item)\n",
    "test_mat = rating_vectorize(ratings_test_dict, n_user, n_item)\n",
    "\n",
    "model = ItemAutoEncoder(n_user, hidden_dim=128)\n",
    "opt = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "USE_CUDA = True\n",
    "N_EPOCHS = 32\n",
    "report_interval=1\n",
    "\n",
    "\n",
    "if USE_CUDA:\n",
    "    compute_device = device('cuda')\n",
    "    model.cuda()\n",
    "    rating_mat = rating_mat.to(compute_device)\n",
    "    test_mat = test_mat.to(compute_device)\n",
    "else:\n",
    "    compute_device = device('cpu')\n",
    "\n",
    "train_loss_by_ep = []\n",
    "test_rmse_by_ep = []\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_data = autorec_data_loader(ratings_train_dict, n_user, n_item)\n",
    "\n",
    "    ep_loss = []\n",
    "    for i, batch in enumerate(train_data):\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = autograd.Variable(batch).to(compute_device)\n",
    "\n",
    "        preds = model(inputs)\n",
    "        loss = maskedSE(input=preds, target=inputs)\n",
    "        loss.mean().backward()\n",
    "        opt.step()\n",
    "        ep_loss.extend(loss.data.to(compute_device).tolist())\n",
    "\n",
    "    train_loss_by_ep.append(np.sqrt(np.mean(ep_loss)))\n",
    "    \n",
    "    # test\n",
    "    preds = model.pred(rating_mat)\n",
    "    test_rmse = maskedSE(preds, test_mat).mean().sqrt()\n",
    "    test_rmse_by_ep.append(test_rmse.data.to(compute_device).tolist())\n",
    "\n",
    "    if report_interval > 0 \\\n",
    "            and ((epoch+1) % report_interval == 0):\n",
    "        \n",
    "        t1=time.time()\n",
    "        print(f'Epoch: {epoch+1}, Time: {round(t1-t0,2)}, /Average train loss {round(sum(train_loss_by_ep[-report_interval:])/report_interval, 5)}')\n",
    "        print(f'\\t\\t\\t/Average test loss {round(sum(test_rmse_by_ep[-report_interval:])/report_interval, 5)}')\n",
    "        t0=time.time()\n",
    "\n",
    "model = model.to('cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00499476119a38fdac92034240d7ef2fa4f5985bf02d398f0fd3693908f0286e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
